{"cells":[{"cell_type":"code","execution_count":null,"id":"e6ed2277","metadata":{"id":"e6ed2277"},"outputs":[],"source":["import sys\n","import os\n","import string\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from collections import Counter\n","import pandas as pd\n","import numpy as np\n","\n","from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woxlpLvw_h6h","executionInfo":{"status":"ok","timestamp":1653682799289,"user_tz":420,"elapsed":28707,"user":{"displayName":"Thomas G","userId":"04453720672694850946"}},"outputId":"74e14681-ec91-4008-8166-0b7998b25729"},"id":"woxlpLvw_h6h","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# FUNCTIONS\n","\n","#defining the function to remove punctuation\n","def clean_text(text):\n","    punctuationfree=\"\".join([i.lower() for i in text if i not in string.punctuation])\n","    return punctuationfree\n","\n","import re\n","def tokenization(text):\n","    tokens = re.split('W+',text)\n","    return tokens\n","\n","# creates word2vec model and returns list of word-vector embeddings\n","# sentences should be a list of lists of tokenized\n","def load_word_vectors(sentences):\n","    model = Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)\n","    word_vectors = model.wv\n","    return word_vectors\n","\n","def build_vocab(corpus):\n","    word_count = {}\n","    for sentence in corpus:\n","        for token in sentence:\n","            if token not in word_count:\n","                word_count[token] = 1\n","            else:\n","                word_count[token] += 1\n","    return word_count\n","\n","def word2index(vocab):\n","    word_index = {w: i for i, w in enumerate(vocab)}\n","    idx_word = {i: w for i, w in enumerate(vocab)}\n","    return word_index, idx_word"],"metadata":{"id":"5Vkd8wQ3EAdz"},"id":"5Vkd8wQ3EAdz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SentencePairs(Dataset):\n","    \n","    def __init__(self, file, transform=None):\n","        self.data = pd.read_pickle(file)\n","\n","        s1_tokens = self.data['s1_tokens']\n","        s2_tokens = self.data['s2_tokens']\n","        corpus = s1_tokens + s2_tokens\n","\n","        self.transform = transform\n","        self.vocab = self.build_vocab(corpus)\n","        self.word2idx, self.idx2word = self.word2index(self.vocab)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        \n","        # get input sentences\n","        tokens1 = self.data['s1_tokens'][idx]\n","        tokens2 = self.data['s2_tokens'][idx]\n","        # convert words to index\n","\n","        input1 = [self.word2idx[word] for word in tokens1]\n","        input2 = [self.word2idx[word] for word in tokens2]\n","        # get label\n","        label = self.data['bin_label'][idx]\n","\n","        # build sample\n","        sample = {\"sentence1\": input1, \"sentence2\": input2, \"label\": label}\n","        return sample\n","\n","    def build_vocab(self, corpus):\n","        word_count = {}\n","        for sentence in corpus:\n","\n","            for token in sentence:\n","                # print(token)\n","                if token not in word_count:\n","                    word_count[token] = 1\n","                else:\n","                    word_count[token] += 1\n","        return word_count\n","\n","    def word2index(self, vocab):\n","        word_index = {w: i for i, w in enumerate(vocab)}\n","        idx_word = {i: w for i, w in enumerate(vocab)}\n","        return word_index, idx_word"],"metadata":{"id":"YV1_-ygWdFR2"},"id":"YV1_-ygWdFR2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mydataset = SentencePairs(file = 'twitter_corpus_clean.pkl')\n","mydataset[0]\n","sample1 = mydataset[0]['sentence1']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2azNxC1uBdA","executionInfo":{"status":"ok","timestamp":1653694392176,"user_tz":420,"elapsed":3163,"user":{"displayName":"Thomas G","userId":"04453720672694850946"}},"outputId":"97ad54ba-ff1a-4937-ad4f-c076c220fee0"},"id":"J2azNxC1uBdA","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 0,\n"," 'sentence1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13],\n"," 'sentence2': [14, 15, 16, 17, 0, 2, 18, 19, 20, 21, 22, 23]}"]},"metadata":{},"execution_count":248}]},{"cell_type":"code","source":["corpus_dir = \"/content/drive/MyDrive/Corpus\"\n","twitter_train = os.path.join(corpus_dir, \"Twitter_URL_Corpus_train.txt\")\n","\n","# read as tsv\n","data = pd.read_csv(twitter_train, sep = \"\\t\", names = ['s1', 's2', 'label', 'url'])"],"metadata":{"id":"q0ntWh1B_2Dh"},"id":"q0ntWh1B_2Dh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# clean text\n","data['s1_clean'] = data['s1'].apply(lambda x: clean_text(x))\n","data['s2_clean'] = data['s2'].apply(lambda x: clean_text(x))\n","\n","# tokenize\n","data['s1_tokens'] = data['s1_clean'].apply(lambda x: x.split())\n","data['s2_tokens'] = data['s2_clean'].apply(lambda x: x.split())\n","\n","# create binary labels, ignoring 3,6 since these are inconclusive\n","data = data[data.label != \"(3,6)\"]\n","data['bin_label'] = data['label'].apply(lambda x: 0 if x in ['(0,6)', '(1,6)', '(2,6)'] else 1)\n","\n","# save to csv\n","data.to_pickle('twitter_corpus_clean.pkl')\n","\n","# TODO: clean/save test data"],"metadata":{"id":"wt9pcwycd9XH"},"id":"wt9pcwycd9XH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: \n","# Load csv\n","data = pd.read_csv('twitter_corpus_clean.csv')\n","# Create vectorized data\n","# Create data loader\n","\n","# compile list of all tokenized sentences\n","s1_tokens = data['s1_tokens'].values.tolist()\n","s2_tokens = data['s2_tokens'].values.tolist()\n","sentence_lists = s1_tokens + s2_tokens\n","\n","# create word_vectors list\n","# vectorized_word = word_vectors[word], can also use list of words\n","word_vectors = load_word_vectors(sentence_lists)\n","\n"],"metadata":{"id":"9-gt3Fz3A2tL"},"id":"9-gt3Fz3A2tL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Approaches  \n","    - sentence embeddings via pretrained BERT model  \n","    - word embeddings -> sentence matrix as input"],"metadata":{"id":"GKxpnMeAwPYe"},"id":"GKxpnMeAwPYe"},{"cell_type":"code","source":[""],"metadata":{"id":"3cLU4hGxgFRi"},"id":"3cLU4hGxgFRi","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4de7d3e1","metadata":{"id":"4de7d3e1"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"2a07f95b","metadata":{"id":"2a07f95b"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"f19af364","metadata":{"id":"f19af364"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"8d7707f5","metadata":{"id":"8d7707f5"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"data_preprocessing.ipynb","provenance":[{"file_id":"1p9j8mhSIF6rrcN0w1r7Dwaz3dXNDEyXK","timestamp":1653695778333}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}