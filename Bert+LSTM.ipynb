{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788d9461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ZW\\anaconda\\envs\\dl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\ZW\\anaconda\\envs\\dl\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, WeightedRandomSampler\n",
    "from transformers import BertTokenizer,BertModel\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0b35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bert_Tokenizer(model_name):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n",
    "tokenizer = Bert_Tokenizer('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2568c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How the metaphors we use to describe discovery affect men and women in the sciences', 'Light Bulbs or Seeds ? How Metaphors for Ideas Influence Judgments About Genius']\n"
     ]
    }
   ],
   "source": [
    "def text_split():\n",
    "    #filename = sys.argv[1]\n",
    "    filename = 'Twitter_URL_Corpus_train.txt'\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    lines = file.readlines()\n",
    "    count = 0\n",
    "    raw = []\n",
    "    label = []\n",
    "    for line in lines:\n",
    "        tokens = line.split('\\t')\n",
    "        if int(tokens[2][1]) !=3:\n",
    "            raw.append([tokens[0].strip(), tokens[1].strip()])\n",
    "            if int(tokens[2][1]) <= 2:  \n",
    "                label.append(0)  \n",
    "            else:\n",
    "                label.append(1)  \n",
    "        count += 1\n",
    "       \n",
    "    return raw, label \n",
    "\n",
    "train, train_label=text_split()\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8018c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ([2129, 1996, 19240, 2015, 2057, 2224, 2000, 6235, 5456, 7461, 2273, 1998, 2308, 1999, 1996, 4163, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100], [2422, 25548, 2030, 8079, 1029, 2129, 19240, 2015, 2005, 4784, 3747, 26186, 2055, 11067, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100], 0))\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, questions_list, tokenizer, labels, max_len):\n",
    "        self.questions_list = questions_list\n",
    "        self.labels = labels\n",
    "        self.bert_encode = tokenizer\n",
    "        \n",
    "#         self.texts = df.text.values\n",
    "#         self.labels = df.target.values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        questions_pair = self.questions_list[index]\n",
    "        q1 = questions_pair[0]\n",
    "        q2 = questions_pair[1]\n",
    "        q1_tokens = self.get_token_mask(q1,self.max_len)\n",
    "        q2_tokens = self.get_token_mask(q2,self.max_len)\n",
    "        \n",
    "        return q1_tokens, q2_tokens, self.labels[index]\n",
    "    \n",
    "    def get_token_mask(self,text,max_len):\n",
    "        \n",
    "        tokens = []\n",
    "        mask = []\n",
    "        text = self.bert_encode.encode(text)\n",
    "        size = len(text)\n",
    "        pads = self.bert_encode.encode(['PAD']*(max(0,max_len-size)))\n",
    "        tokens[:max(max_len,size)] = text[:max(max_len,size)]\n",
    "        tokens = tokens + pads[1:-1]\n",
    "        mask = [1]*size+[0]*len(pads[1:-1])\n",
    "        tokens_len = len(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "train_dataset = CustomDataset(train,tokenizer, train_label, 120)\n",
    "print(next(enumerate(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09691f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollate:\n",
    "    def custom_collate(self, batch):\n",
    "\n",
    "        # batch = list of tuples where each tuple is of the form ([i1, i2, i3], [j1, j2, j3], label)\n",
    "        q1_list = []\n",
    "        q2_list = []\n",
    "        labels = []\n",
    "        for training_example in batch:\n",
    "#             print(batch)\n",
    "            q1_list.append(training_example[0])\n",
    "            q2_list.append(training_example[1])\n",
    "            labels.append(training_example[2])\n",
    "          \n",
    "        \n",
    "        q1_lengths = [len(q) for q in q1_list]\n",
    "        q2_lengths = [len(q) for q in q2_list]\n",
    "        \n",
    "        return q1_list, q1_lengths, q2_list, q2_lengths, labels\n",
    "#         return q1_list, q2_list, labels\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.custom_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1091ba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ([[2129, 2865, 11730, 2024, 9992, 4214, 3889, 7221, 8540, 1037, 16939, 17678, 16098, 16089, 3367, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100], [7279, 3401, 7283, 2635, 2058, 6653, 3947, 2013, 13144, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]], [118, 118], [[1996, 4037, 8945, 2075, 5092, 2075, 2234, 2039, 2007, 1037, 6179, 6994, 2005, 1996, 8398, 3690, 2006, 9857, 1024, 1037, 2317, 22006, 7327, 8458, 23238, 6491, 13103, 2005, 8845, 1012, 1996, 2609, 3964, 1056, 1012, 1012, 1012, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100], [19075, 2003, 2008, 1030, 18079, 26654, 2666, 2001, 2183, 2000, 3288, 1999, 1037, 9129, 1997, 5747, 1011, 3690, 9253, 8663, 2015, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]], [118, 118], [0, 0])\n",
      "Training Set Size 33760, Validation Set Size 8440\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.2\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "shuffle_dataset = True\n",
    "random_seed = 32\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# print(train_indices)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "# print(next(enumerate(train_sampler)))\n",
    "validation_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, sampler=train_sampler, collate_fn=CustomCollate())\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, sampler=validation_sampler, collate_fn=CustomCollate())\n",
    "\n",
    "for i, (q1_list, q1_lengths, q2_list, q2_lengths, labels) in enumerate(train_loader):\n",
    "    print(i, (q1_list, q1_lengths, q2_list, q2_lengths, labels))\n",
    "    break\n",
    "print ('Training Set Size {}, Validation Set Size {}'.format(len(train_indices), len(val_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b161cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_REQUIRES_GRAD = False\n",
    "# HIDDEN_CELLS = 25\n",
    "# NUM_LAYERS = 1\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self,pre_trained='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(pre_trained)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.lstm = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n",
    "        self.clf = nn.Linear(self.hidden_size*2,1)\n",
    "    \n",
    "    # Manhattan Distance Calculator\n",
    "    def exponent_neg_manhattan_distance(self, x1, x2):\n",
    "        return torch.exp(-torch.sum(torch.abs(x1 - x2), dim=0))\n",
    "\n",
    "    def forward_once(self,inputs, input_length):\n",
    "        encoded_layers, pooled_output = self.bert(input_ids=torch.tensor(inputs))\n",
    "        encoded_layers = encoded_layers.permute(1, 0, 2)\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(encoded_layers, input_length))\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "#         output_hidden = F.dropout(output_hidden,0.2)\n",
    "        output = self.clf(output_hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def forward(self, q1, q1_lengths, q2, q2_lengths):\n",
    "        output1 = self.forward_once(q1, q1_lengths)\n",
    "        print('----------q1----------')\n",
    "        print(output1)\n",
    "        output2 = self.forward_once(q2, q2_lengths)\n",
    "        print('----------q2----------')\n",
    "        similarity_score = torch.zeros(output1.size()[0])\n",
    "        # Calculate Similarity Score between both questions in a single pair\n",
    "        for index in range(output1.size()[0]):\n",
    "            # Sequence lenghts are being used to index and retrieve the activations before the zero padding since they were not part of original question\n",
    "            q1 = output1[index, q1_lengths[index] - 1, :]\n",
    "#             print('oh')\n",
    "#             print(q1.size())\n",
    "            q2 = output2[index, q2_lengths[index] - 1, :]\n",
    "#             print('ho')\n",
    "#             print(q2.size())\n",
    "            similarity_score[index] = self.exponent_neg_manhattan_distance(q1, q2)\n",
    "        return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9997c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------q1----------\n",
      "tensor([[-0.1724],\n",
      "        [-0.2094]], grad_fn=<AddmmBackward0>)\n",
      "----------q2----------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-51f7283c5de7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Run the forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0msimilarity_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq1_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq1_batch_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2_batch_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ZW\\anaconda\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-b80a02e12dc7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, q1, q1_lengths, q2, q2_lengths)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# Sequence lenghts are being used to index and retrieve the activations before the zero padding since they were not part of original question\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mq1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq1_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;31m#             print('oh')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#             print(q1.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "model = CustomNetwork()\n",
    "\n",
    "total_step = len(train_loader)\n",
    "# Threshold 0.5. Since similarity score will be a value between 0 and 1, we will consider all question pair with values greater than threshold as Duplicate\n",
    "threshold = torch.Tensor([0.5])\n",
    "\n",
    "# define hyperparameter\n",
    "num_epochs = 1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001 )\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_history = []\n",
    "    model.train(True)\n",
    "    train_correct_total = 0\n",
    "    for i, (q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths, labels) in enumerate(train_loader):\n",
    "#         print(labels)\n",
    "        labels = torch.FloatTensor(labels)\n",
    "        \n",
    "        # Clear grads\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run the forward pass\n",
    "        similarity_score = model(q1_batch, q1_batch_lengths, q2_batch, q2_batch_lengths)\n",
    "        print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f4f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
